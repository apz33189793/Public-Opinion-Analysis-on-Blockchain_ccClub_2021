{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff57f599-7714-48e9-a6a1-d16d393334cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23fd1e-10e8-43da-8cb8-e0c7c2132371",
   "metadata": {},
   "source": [
    "### 動區(BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efc0b3-4821-4277-abb1-739f46e2e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "title = []\n",
    "website = []\n",
    "content = []\n",
    "\n",
    "pages = 372\n",
    "for page in range(1, pages+1):\n",
    "    print(page)\n",
    "    if page == pages:\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(3)\n",
    "        url = f\"https://www.blocktempo.com/2021/page/{page}/\"\n",
    "        re = requests.get(url) \n",
    "        soup_blocktempo =  BeautifulSoup(re.text, \"html.parser\")\n",
    "        \n",
    "        for j in soup_blocktempo.find_all(\"h3\", {\"class\": \"jeg_post_title\"}):\n",
    "            url_article = j.a['href']\n",
    "            if url_article not in website:\n",
    "                website.append(url_article)  #存網址\n",
    "                print(url_article)\n",
    "                re_article = requests.get(url_article)\n",
    "                soup_article = BeautifulSoup(re_article.text, \"html.parser\")\n",
    "                for t in soup_article.findAll(\"div\", {\"class\": \"entry-header\"}):\n",
    "                    t2 = t.h1.text\n",
    "                    print(t2)\n",
    "                    title.append(t2)  #存標題\n",
    "                for c in soup_article.findAll(\"div\", {\"class\": \"content-inner\"}):\n",
    "                    c2 = c.text\n",
    "                    print(c2)\n",
    "                    content.append(c2)  #存文章\n",
    "                for d in soup_article.findAll(\"div\", {\"class\": \"jeg_meta_date\"}):\n",
    "                    d2 = d.find('a').text\n",
    "                    print(d2)\n",
    "                    date.append(d2)  #存日期\n",
    "                    break\n",
    "#                 time.sleep(2)\n",
    "\n",
    "#建dataframe\n",
    "df_blocktempo = pd.DataFrame(list(zip(date, title, website, content)), columns = ['日期', '標題', '網址', '內容'])\n",
    "\n",
    "#刪除文末不相關資訊\n",
    "for k in range(len(content)):\n",
    "    for i, j in zip(range(len(content[k])), content[k]):\n",
    "        if j =='📍':\n",
    "            df_blocktempo['內容'][k] = content[k][0:i-5]\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12347e96-8188-490a-9e94-5417bcb3106d",
   "metadata": {},
   "source": [
    "### 鉅亨網(json+BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58cb2e-1b57-4642-86f4-debb3dec066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_article_cnyes = []\n",
    "date_cnyes = []\n",
    "title_cnyes = []\n",
    "content_cnyes = []\n",
    "\n",
    "user_agent_cnyes = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent_cnyes}\n",
    "\n",
    "pages_cnyes = 55\n",
    "for i in range(1,pages_cnyes): \n",
    "    url_cnyes = f'https://api.cnyes.com/media/api/v1/newslist/category/bc?startAt=1609430400&endAt=1639929599&limit=30&page={i}'\n",
    "    time.sleep(3)\n",
    "    data = requests.get(url_cnyes, headers = headers).json()\n",
    "    for j in range(len(data['items']['data'])):\n",
    "        newsId = data['items']['data'][j]['newsId']\n",
    "        url_a = f'https://news.cnyes.com/news/id/{newsId}?exp=a'\n",
    "        url_article_cnyes.append(url_a)  #存網址\n",
    "url_article_cnyes.remove('https://news.cnyes.com/news/id/4787036?exp=a') #原文無內容\n",
    "\n",
    "for url_a in url_article_cnyes:\n",
    "#     time.sleep(2)\n",
    "    re_article = requests.get(url_a)\n",
    "    soup_article = BeautifulSoup(re_article.text, \"html.parser\")\n",
    "    for t in soup_article.findAll(\"h1\", {\"itemprop\": \"headline\"}):\n",
    "        t2 = t.text\n",
    "        print(t2)\n",
    "        print(url_a)\n",
    "        title_cnyes.append(t2)  #存標題\n",
    "    for d in soup_article.findAll(\"div\", {\"class\": \"_1R6L\"}):\n",
    "        d2 = d.time.text[0:10].replace('/', '-')\n",
    "        print(d2)\n",
    "        date_cnyes.append(d2)  #存日期\n",
    "    for c in soup_article.findAll(\"div\", {\"class\": \"_2E8y\"}):\n",
    "        c2 = c.text\n",
    "        c2 = c2.replace('原文連結', '')  #刪除文末原文連結\n",
    "        print(c2)\n",
    "        if c2 == '':\n",
    "            pass\n",
    "        else:\n",
    "            content_cnyes.append(c2)  #存文章\n",
    "\n",
    "#建dataframe\n",
    "df_cnyes = pd.DataFrame(list(zip(date_cnyes, title_cnyes, url_article_cnyes, content_cnyes)), columns = ['日期', '標題', '網址', '內容'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497f215-e179-4739-ab0c-2bdf6407a0bd",
   "metadata": {},
   "source": [
    "### 手榴彈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f462b3-b22e-4f1a-b565-9b4e9954c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c291d-362e-42e5-b0f8-1cde274655dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.grenade.tw/blog/\"\n",
    "re = requests.get(url)\n",
    "soup_grenade = BeautifulSoup(re.text,\"html.parser\")\n",
    "all_bloglist = soup_grenade.findAll(\"div\", {\"class\":\"elementor-post__text\"})\n",
    "for blog in all_bloglist:\n",
    "#    print(blog.a.text)\n",
    "#    print(blog.a[\"href\"])\n",
    "    url_blog = blog.a[\"href\"]\n",
    "    re_blog = requests.get(url_blog)\n",
    "    soup_blog = BeautifulSoup(re_blog.text,\"html.parser\")\n",
    "    for articals in soup_blog.find_all(\"div\", {\"class\":\"content-area\"}):\n",
    "        artical = articals.text\n",
    "        print(artical)\n",
    "\n",
    "date = []\n",
    "title = []\n",
    "website = []\n",
    "content = []\n",
    "\n",
    "pages = 44\n",
    "for page in range(1, pages+1):\n",
    "    print(page)\n",
    "    if page == pages:\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(3)  #休息時間，掛掉的話調整秒數\n",
    "        url = f\"https://www.grenade.tw/blog/{page}/\"\n",
    "        re = requests.get(url) \n",
    "        soup_grenade =  BeautifulSoup(re.text, \"html.parser\")\n",
    "        for blog in soup_grenade.findAll(\"div\", {\"class\":\"elementor-post__text\"}):\n",
    "            url_blog = blog.a[\"href\"]\n",
    "            if url_blog not in website:\n",
    "                website.append(url_blog)  #存網址\n",
    "                print(url_blog)\n",
    "                re_blog = requests.get(url_blog)\n",
    "                soup_blog = BeautifulSoup(re_blog.text,\"html.parser\")\n",
    "                for articals in soup_blog.find_all(\"div\", {\"class\":\"content-area\"}):\n",
    "                    artical = articals.text\n",
    "                    print(artical)\n",
    "                    content.append(artical) #存文章\n",
    "                for titles in soup_blog.find_all(\"h1\", {\"class\":\"entry-title\"}):\n",
    "                    title1 = titles.text\n",
    "                    print(title)            \n",
    "                    title.append(title1)    #存標題\n",
    "                for dates in soup_blog.find_all(\"time\", {\"class\":\"entry-date published\"}):\n",
    "                    dately = dates.text\n",
    "                    print(dately)\n",
    "                    date.append(dately)    #存日期\n",
    "                time.sleep(0.5)\n",
    "#建dataframe\n",
    "df_grenade = pd.DataFrame(list(zip(date, title, website, content)), columns = ['日期', '標題', '網址', '內容'])\n",
    "#刪除文末不相關資訊\n",
    "for k in range(len(content)):\n",
    "    for i, j in zip(range(len(content[k])), content[k]):\n",
    "        if j =='📍':\n",
    "            df_blocktempo['內容'][k] = content[k][0:i-5]\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc743565-652c-4b51-a6f9-c457058a42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_grenade[\"日期\"]:\n",
    "    df_grenade[\"日期\"] = i.replace(\" 月\", \"\")\n",
    "newgrenade = pd.read_csv(\"/Users/gaozhichien/2021Fall_ccClub/project/grenade.csv\")\n",
    "lst = []\n",
    "lst1 = []\n",
    "from datetime import datetime\n",
    "for i in newgrenade[\"日期\"]:\n",
    "    新日期 = i.replace(\" 月\", \"\")\n",
    "    lst.append(新日期)\n",
    "for j in lst:\n",
    "    aa = str(datetime.strptime(j, \"%d %m, %Y\")).replace(\" 00:00:00\",\"\")\n",
    "    lst1.append(aa)\n",
    "df = pd.DataFrame(lst1)\n",
    "df.set_axis(['日期'], axis='columns', inplace=True)\n",
    "from datetime import datetime\n",
    "for i in df_grenade[\"日期\"]:\n",
    "    新日期 = i.replace(\" 月\", \"\")\n",
    "    lst.append(新日期)\n",
    "for j in lst:\n",
    "    aa = str(datetime.strptime(j, \"%d %m, %Y\")).replace(\" 00:00:00\",\"\")\n",
    "    lst1.append(aa)\n",
    "df = pd.DataFrame(lst1)\n",
    "df.set_axis(['日期'], axis='columns', inplace=True)\n",
    "df_grenade[\"日期\"]  = df[\"日期\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d20a73-ff79-4025-8227-c9237269d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result ='/Users/gaozhichien/2021Fall_ccClub/project/df_grenade.csv'\n",
    "\n",
    "df_grenade = pd.DataFrame.from_dict( df_grenade )\n",
    "df_grenade.to_csv( Result  , index=False )\n",
    "    \n",
    "print( '成功產出' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a8b6b-4ef9-4e00-90a0-375de187a82a",
   "metadata": {},
   "source": [
    "### 桑幣(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeafccd-8686-4eb0-9be5-316df6ace1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "'user-agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\",\n",
    "\"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "\n",
    "def clean_str(x):\n",
    "    x=x.text.strip()\n",
    "    x=x.replace(\"\\n\",\"\")\n",
    "    x=x.split(\"📖加入桑幣的社群平\")\n",
    "    return x[0]\n",
    "url_lst=[]\n",
    "topic_lst=[]\n",
    "date_lst=[]\n",
    "content_lst=[]\n",
    "for i in range(1,28):\n",
    "    url=f\"https://zombit.info/category/%e7%86%b1%e9%96%80%e6%99%82%e4%ba%8b/page/{i}/\"\n",
    "    \n",
    "    re = requests.get(url,headers=headers)\n",
    "    print(url,re.status_code)\n",
    "    if re.status_code == requests.codes.ok:\n",
    "        soup = BeautifulSoup(re.text, 'lxml')\n",
    "        al = soup.find('div', {'class': \"jeg_main_content jeg_column col-sm-8\"})\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "        titles=al.find_all('h3',{'class':'jeg_post_title'})\n",
    "\n",
    "        for title in titles:\n",
    "            url1 = title.a[\"href\"]\n",
    "            topic = title.find(\"a\").text\n",
    "            url_lst.append(url1)\n",
    "            topic_lst.append(topic)\n",
    "            #內容和日期\n",
    "            re = requests.get(url1,headers=headers) \n",
    "            soup = BeautifulSoup(re.text, 'lxml')\n",
    "\n",
    "            content=soup.find(\"div\",class_=\"elementor-widget-wrap\")\n",
    "            content=clean_str(content)\n",
    "            content_lst.append(content)\n",
    "\n",
    "            date=soup.find(\"div\",class_=\"jeg_meta_date\")\n",
    "            date=date.text.strip()\n",
    "            date_lst.append(date)\n",
    "\n",
    "\n",
    "        dic={\"date\":date_lst,\"topic\":topic_lst,\"url\":url_lst,\"content\":content_lst}\n",
    "        data_frame = pd.DataFrame(dic)\n",
    "#         print(data_frame )\n",
    "    else:\n",
    "        print(\"QQ\")\n",
    "data_frame.columns = ['日期','標題','網址','內容']\n",
    "data_frame.to_csv('/Users/flora/ccClub/zombit.csv',index=False, encoding='utf-8')  \n",
    "len(data_frame.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a2a49-f874-4a56-8a33-f6d771a7b2f4",
   "metadata": {},
   "source": [
    "### 合併檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98923b84-89a4-4080-bab6-e76382801e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "original = pd.read_csv(\"/Users/gaozhichien/2021Fall_ccClub/project/df_zz.csv\")\n",
    "newgrenade = pd.read_csv(\"/Users/gaozhichien/2021Fall_ccClub/project/newgrenade.csv\")\n",
    "bee = pd.read_csv(\"/Users/gaozhichien/2021Fall_ccClub/project/zombit.csv\")\n",
    "chain = pd.concat([original,newgrenade])\n",
    "chain1 = pd.concat([chain,bee])\n",
    "chain1 = pd.read_csv(\"/Users/gaozhichien/2021Fall_ccClub/project/chain1.csv\")\n",
    "\n",
    "lst = []\n",
    "from datetime import datetime\n",
    "for i in chain1[\"日期\"]:\n",
    "    aa = datetime.strptime(i, \"%Y-%m-%d\")\n",
    "    lst.append(aa)\n",
    "df = pd.DataFrame(lst)\n",
    "df.set_axis(['日期'], axis='columns', inplace=True)\n",
    "chain1[\"日期\"] = df[\"日期\"]\n",
    "chain11 = chain1.sort_values(by=['日期'])\n",
    "chain_date = pd.DataFrame(chain11)\n",
    "chain_date.reset_index(drop = True, inplace = True)\n",
    "\n",
    "Result ='/Users/gaozhichien/2021Fall_ccClub/project/chain_date.csv'\n",
    "\n",
    "chain_date = pd.DataFrame.from_dict( chain_date )\n",
    "chain_date.to_csv( Result  , index=False )\n",
    "    \n",
    "print( '成功產出' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
